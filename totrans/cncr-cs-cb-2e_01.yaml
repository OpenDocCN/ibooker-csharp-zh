- en: 'Chapter 1\. Concurrency: An Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is a key aspect of beautiful software. For decades, concurrency
    was possible but difficult to achieve. Concurrent software was difficult to write,
    difficult to debug, and difficult to maintain. As a result, many developers chose
    the easier path and avoided concurrency. With the libraries and language features
    available for modern .NET programs, concurrency is now much easier. Microsoft
    has led the way in significantly lowering the bar for concurrency. Previously,
    concurrent programming was the domain of experts; these days, every developer
    can (and should) embrace concurrency.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Concurrency
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before continuing, I’d like to clear up some terminology that I’ll be using
    throughout this book. These are my own definitions that I use consistently to
    disambiguate different programming techniques. Let’s start with *concurrency*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Doing more than one thing at a time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: I hope it’s obvious how concurrency is helpful. End-user applications use concurrency
    to respond to user input *while* writing to a database. Server applications use
    concurrency to respond to a second request *while* finishing the first request.
    You need concurrency any time you need an application to do one thing *while*
    it’s working on something else. Almost every software application in the world
    can benefit from concurrency.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Most developers hearing the term “concurrency” immediately think of “multithreading.”
    I’d like to draw a distinction between these two.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A form of concurrency that uses multiple threads of execution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading refers to literally using multiple threads. As demonstrated in
    many recipes in this book, multithreading is *one* form of concurrency, but certainly
    not the only one. In fact, direct use of low-level threading types has almost
    no purpose in a modern application; higher-level abstractions are more powerful
    and more efficient than old-school multithreading. For that reason, I’ll minimize
    my coverage of outdated techniques. None of the multithreading recipes in this
    book use the `Thread` or `BackgroundWorker` types; they have been replaced with
    superior alternatives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As soon as you type `new Thread()`, it’s over; your project already has legacy
    code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'But don’t get the idea that multithreading is dead! Multithreading lives on
    in the *thread pool*, a useful place to queue work that automatically adjusts
    itself according to demand. In turn, the thread pool enables another important
    form of concurrency: *parallel processing*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Doing lots of work by dividing it up among multiple threads that run concurrently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing (or parallel programming) uses multithreading to maximize
    the use of multiple processor cores. Modern CPUs have multiple cores, and if there’s
    a lot of work to do, then it makes no sense to make one core do all the work while
    the others sit idle. Parallel processing splits the work among multiple threads,
    which can each run independently on a different core.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel processing is one type of multithreading, and multithreading is one
    type of concurrency. There’s another type of concurrency that is important in
    modern applications but isn’t as familiar to many developers: *asynchronous programming*.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A form of concurrency that uses futures or callbacks to avoid unnecessary threads.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'A *future* (or *promise*) is a type that represents some operation that will
    complete in the future. Some modern future types in .NET are `Task` and `Task<TResult>`.
    Older asynchronous APIs use callbacks or events instead of futures. Asynchronous
    programming is centered around the idea of an *asynchronous operation*: some operation
    that is started that will complete some time later. While the operation is in
    progress, it doesn’t block the original thread; the thread that starts the operation
    is free to do other work. When the operation completes, it notifies its future
    or invokes its callback or event to let the application know the operation is
    finished.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming is a powerful form of concurrency, but until recently,
    it required extremely complex code. The `async` and `await` support in modern
    languages make asynchronous programming almost as easy as synchronous (nonconcurrent)
    programming.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Another form of concurrency is *reactive programming*. Asynchronous programming
    implies that the application will start an operation that will complete once at
    a later time. Reactive programming is closely related to asynchronous programming
    but is built on *asynchronous events* instead of *asynchronous operations*. Asynchronous
    events may not have an actual “start,” may happen at any time, and may be raised
    multiple times. One example is user input.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Reactive programming
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: A declarative style of programming where the application reacts to events.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: If you consider an application to be a massive state machine, the application’s
    behavior can be described as reacting to a series of events by updating its state
    at each event. This isn’t as abstract or theoretical as it sounds; modern frameworks
    make this approach quite useful in real-world applications. Reactive programming
    isn’t necessarily concurrent, but it is closely related to concurrency, so this
    book covers the basics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a mixture of techniques is used when writing a concurrent program.
    Most applications at least use multithreading (via the thread pool) and asynchronous
    programming. Feel free to mix and match all the various forms of concurrency,
    using the appropriate tool for each part of the application.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Asynchronous Programming
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Asynchronous programming has two primary benefits. The first benefit is for
    end-user GUI programs: asynchronous programming enables responsiveness. Everyone
    has used a program that temporarily locks up while it’s working; an asynchronous
    program can remain responsive to user input while it’s working. The second benefit
    is for server-side programs: asynchronous programming enables scalability. A server
    application can scale somewhat just by using the thread pool, but an asynchronous
    server application can usually scale an order of magnitude better than that.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程有两个主要好处。第一个好处是对于终端用户GUI程序：异步编程能够提升响应性。每个人都使用过一个在工作时暂时锁定的程序；异步程序可以在工作时保持对用户输入的响应。第二个好处是对于服务器端程序：异步编程能够提升可伸缩性。服务器应用程序可以通过使用线程池来实现一定程度的扩展，但异步服务器应用程序通常可以比这更好地扩展一个数量级。
- en: 'Both benefits of asynchronous programming derive from the same underlying aspect:
    asynchronous programming frees up a thread. For GUI programs, asynchronous programming
    frees up the UI thread; this permits the GUI application to remain responsive
    to user input. For server applications, asynchronous programming frees up request
    threads; this permits the server to use its threads to serve more requests.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程的两个好处都源自同一个基本方面：异步编程释放了一个线程。对于GUI程序，异步编程释放了UI线程；这使得GUI应用程序可以保持对用户输入的响应。对于服务器应用程序，异步编程释放了请求线程；这使得服务器可以使用其线程来处理更多的请求。
- en: 'Modern asynchronous .NET applications use two keywords: `async` and `await`.
    The `async` keyword is added to a method declaration, and performs a double purpose:
    it enables the `await` keyword within that method and it signals the compiler
    to generate a state machine for that method, similar to how `yield return` works.
    An `async` method may return `Task<TResult>` if it returns a value, `Task` if
    it doesn’t return a value, or any other “task-like” type, such as `ValueTask`.
    In addition, an `async` method may return `IAsyncEnumerable<T>` or `IAsyncEnumerator<T>`
    if it returns multiple values in an enumeration. The task-like types represent
    futures; they can notify the calling code when the `async` method completes.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现代异步.NET应用程序使用两个关键字：`async`和`await`。`async`关键字添加到方法声明中，起到双重作用：它在该方法内启用`await`关键字，并提示编译器为该方法生成状态机，类似于`yield
    return`的工作方式。如果异步方法返回值，它可以返回`Task<TResult>`，如果不返回值，则返回`Task`或任何其他“类似任务”的类型，如`ValueTask`。此外，如果异步方法返回枚举中的多个值，则可以返回`IAsyncEnumerable<T>`或`IAsyncEnumerator<T>`。类似任务的类型代表未来；它们可以在异步方法完成时通知调用代码。
- en: Warning
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Avoid `async void`! It is possible to have an `async` method return `void`,
    but you should only do this if you’re writing an `async` event handler. A regular
    `async` method without a return value should return `Task`, not `void`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用`async void`！可能有一个`async`方法返回`void`，但只有在编写`async`事件处理程序时才应该这样做。没有返回值的常规`async`方法应该返回`Task`，而不是`void`。
- en: 'With that background, let’s take a quick look at an example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，让我们快速看一个例子：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: An `async` method begins executing synchronously, just like any other method.
    Within an `async` method, the `await` keyword performs an *asynchronous wait*
    on its argument. First, it checks whether the operation is already complete; if
    it is, it continues executing (synchronously). Otherwise, it will pause the `async`
    method and return an incomplete task. When that operation completes some time
    later, the `async` method will resume executing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`async`方法开始同步执行，就像任何其他方法一样。在`async`方法内部，`await`关键字对其参数执行*异步等待*。首先，它检查操作是否已经完成；如果完成，它将继续执行（同步）。否则，它将暂停`async`方法并返回一个未完成的任务。当操作稍后完成时，`async`方法将继续执行。'
- en: You can think of an `async` method as having several synchronous portions, broken
    up by `await` statements. The first synchronous portion executes on whatever thread
    calls the method, but where do the other synchronous portions execute? The answer
    is a bit complicated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把`async`方法看作有几个同步部分，这些部分由`await`语句分隔开。第一个同步部分在调用方法的任何线程上执行，但其他同步部分在哪里执行呢？答案有点复杂。
- en: When you `await` a task (the most common scenario), a *context* is captured
    when the `await` decides to pause the method. This is the current `SynchronizationContext`
    unless it’s `null`, in which case the context is the current `TaskScheduler`.
    The method resumes executing within that captured context. Usually, this context
    is the UI context (if you’re on the UI thread) or the threadpool context (most
    other situations). If you have an ASP.NET Classic (pre-Core) application, then
    the context could also be an ASP.NET request context. ASP.NET Core uses the threadpool
    context rather than a special request context.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: So, in the preceding code, all the synchronous portions will attempt to resume
    on the original context. If you call `DoSomethingAsync` from a UI thread, each
    of its synchronous portions will run on that UI thread; but if you call it from
    a threadpool thread, each of its synchronous portions will run on any threadpool
    thread.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'You can avoid this default behavior by awaiting the result of the `ConfigureAwait`
    extension method and passing `false` for the `continueOnCapturedContext` parameter.
    The following code will start on the calling thread, and after it is paused by
    an `await`, it’ll resume on a threadpool thread:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s good practice to always call `ConfigureAwait` in your core “library” methods,
    and only resume the context when you need it—in your outer “user interface” methods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The `await` keyword is not limited to working with tasks; it can work with any
    kind of *awaitable* that follows a certain pattern. As an example, the Base Class
    Library includes the `ValueTask<T>` type, which reduces memory allocations if
    the result is commonly synchronous; for example, if the result can be read from
    an in-memory cache. `ValueTask<T>` is not directly convertible to `Task<T>`, but
    it does follow the awaitable pattern, so you can directly `await` it. There are
    other examples, and you can build your own, but most of the time `await` will
    take a `Task` or `Task<TResult>`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic ways to create a `Task` instance. Some tasks represent actual
    code that a CPU has to execute; these computational tasks should be created by
    calling `Task.Run` (or `TaskFactory.StartNew` if you need them to run on a particular
    scheduler). Other tasks represent a *notification*; these kinds of event-based
    tasks are created by `TaskCompletionSource<TResult>` (or one of its shortcuts).
    Most I/O tasks use `TaskCompletionSource<TResult>`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Error handling is natural with `async` and `await`. In the code snippet that
    follows, `PossibleExceptionAsync` may throw a `NotSupportedException`, but `TrySomethingAsync`
    can catch the exception naturally. The caught exception has its stack trace properly
    preserved and isn’t artificially wrapped in a `TargetInvocationException` or `AggregateException`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When an `async` method throws (or propagates) an exception, the exception is
    placed on its returned `Task` and the `Task` is completed. When that `Task` is
    awaited, the `await` operator will retrieve that exception and (re)throw it in
    a way such that its original stack trace is preserved. Thus, code such as the
    following example would work as expected if `PossibleExceptionAsync` was an `async`
    method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There’s one other important guideline when it comes to `async` methods: once
    you start using `async`, it’s best to allow it to grow through your code. If you
    call an `async` method, you should (eventually) `await` the task it returns. Resist
    the temptation to call `Task.Wait`, `Task<TResult>.Result`, or `GetAwaiter().GetResult()`;
    doing so could cause a deadlock. Consider the following method:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The code in this example will deadlock if called from a UI or ASP.NET Classic
    context because both of those contexts only allow one thread in at a time. `Deadlock`
    will call `WaitAsync`, which begins the delay. `Deadlock` then (synchronously)
    waits for that method to complete, blocking the context thread. When the delay
    completes, `await` attempts to resume `WaitAsync` within the captured context,
    but it cannot because there’s already a thread blocked in the context, and the
    context only allows one thread at a time. Deadlock can be prevented two ways:
    you can use `ConfigureAwait(false)` within `WaitAsync` (which causes `await` to
    ignore its context), or you can `await` the call to `WaitAsync` (making `Deadlock`
    into an `async` method).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you use `async`, it’s best to use `async` all the way.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: For a more complete introduction to `async`, the online documentation that Microsoft
    has provided for `async` is fantastic; I recommend reading at least the [Asynchronous
    Programming overview](http://bit.ly/async-prog) and the [Task-based Asynchronous
    Pattern (TAP)](http://bit.ly/task-async-patt) overview. If you want to go a bit
    deeper, there’s also the [Async in Depth](http://bit.ly/async-indepth) documentation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous streams take the groundwork of `async` and `await` and extend it
    to handle multiple values. Asynchronous streams are built around the concept of
    asynchronous enumerables, which are like regular enumerables, except that they
    enable asynchronous work to be done when retrieving the next item in the sequence.
    This is an extremely powerful concept that [Chapter 3](ch03.html#async-streams)
    covers in more detail. Asynchronous streams are especially useful whenever you
    have a sequence of data that arrives either one at a time or in chunks. For example,
    if your application processes the response of an API that uses paging with `limit`
    and `offset` parameters, then asynchronous streams are an ideal abstraction. As
    of the time of this writing, asynchronous streams are only available on the newest
    .NET platforms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Parallel Programming
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel programming should be used any time you have a fair amount of computation
    work that can be split up into independent chunks. Parallel programming increases
    the CPU usage temporarily to improve throughput; this is desirable on client systems
    where CPUs are often idle, but it’s usually not appropriate for server systems.
    Most servers have some parallelism built in; for example, ASP.NET will handle
    multiple requests in parallel. Writing parallel code on the server may still be
    useful in some situations (if you *know* that the number of concurrent users will
    always be low), but in general, parallel programming on the server would work
    against its built-in parallelism and therefore wouldn’t provide any real benefit.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two forms of parallelism: *data parallelism* and *task parallelism*.
    Data parallelism is when you have a bunch of data items to process, and the processing
    of each piece of data is mostly independent from the other pieces. Task parallelism
    is when you have a pool of work to do, and each piece of work is mostly independent
    from the other pieces. Task parallelism may be dynamic; if one piece of work results
    in several additional pieces of work, they can be added to the pool of work.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few different ways to do data parallelism. `Parallel.ForEach` is
    similar to a `foreach` loop and should be used when possible. `Parallel.ForEach`
    is covered in [Recipe 4.1](ch04.html#recipe-parallel-foreach). The `Parallel`
    class also supports `Parallel.For`, which is similar to a `for` loop, and can
    be used if the data processing depends on the index. Code that uses `Parallel.ForEach`
    looks like the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Another option is PLINQ (Parallel LINQ), which provides an `AsParallel` extension
    method for LINQ queries. `Parallel` is more resource friendly than PLINQ; `Parallel`
    will play more nicely with other processes in the system, while PLINQ will (by
    default) attempt to spread itself over all CPUs. The downside to `Parallel` is
    that it’s more explicit; PLINQ in many cases has more elegant code. PLINQ is covered
    in [Recipe 4.5](ch04.html#recipe-parallel-plinq) and looks like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Regardless of the method you choose, one guideline stands out when doing parallel
    processing.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The chunks of work should be as independent from one another as possible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: As long as your chunk of work is independent from all other chunks, you maximize
    your parallelism. As soon as you start sharing state between multiple threads,
    you have to synchronize access to that shared state, and your application becomes
    less parallel. [Chapter 12](ch12.html#synchronization) covers synchronization
    in more detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The output of your parallel processing can be handled in various ways. You can
    place the results in some kind of a concurrent collection, or you can aggregate
    the results into a summary. Aggregation is common in parallel processing; this
    kind of map/reduce functionality is also supported by the `Parallel` class method
    overloads. [Recipe 4.2](ch04.html#recipe-parallel-aggregate) looks at aggregation
    in more detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s turn to task parallelism. Data parallelism is focused on processing
    data; task parallelism is just about doing work. At a high level, data parallelism
    and task parallelism are similar; “processing data” is a kind of “work.” Many
    parallelism problems can be solved either way; it’s convenient to use whichever
    API is more natural for the problem at hand.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '`Parallel.Invoke` is one type of `Parallel` method that does a kind of fork/join
    task parallelism. This method is covered in [Recipe 4.3](ch04.html#recipe-parallel-invoke);
    you just pass in the delegates you want to execute in parallel:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `Task` type was originally introduced for task parallelism, though these
    days it’s also used for asynchronous programming. A `Task` instance—as used in
    task parallelism—represents some work. You can use the `Wait` method to wait for
    a task to complete, and you can use the `Result` and `Exception` properties to
    retrieve the results of that work. Code using `Task` directly is more complex
    than code using `Parallel`, but it can be useful if you don’t know the structure
    of the parallelism until runtime. With this kind of dynamic parallelism, you don’t
    know how many pieces of work you need to do at the beginning of the processing;
    you find out as you go along. Generally, a dynamic piece of work should start
    whatever child tasks it needs and then wait for them to complete. The `Task` type
    has a special flag, `TaskCreationOptions.AttachedToParent`, which you could use
    for this. Dynamic parallelism is covered in [Recipe 4.4](ch04.html#recipe-parallel-dynamicparallelism).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Task parallelism should strive to be independent, just like data parallelism.
    The more independent your delegates can be, the more efficient your program can
    be. Also, if your delegates aren’t independent, then they need to be synchronized,
    and it’s harder to write correct code if that code needs synchronization. With
    task parallelism, be especially careful of variables captured in closures. Remember
    that closures capture references (not values), so you can end up with sharing
    that isn’t obvious.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Error handling is similar for all kinds of parallelism. Because operations
    are proceeding in parallel, it’s possible for multiple exceptions to occur, so
    they are wrapped up in an `AggregateException` that’s thrown to your code. This
    behavior is consistent across `Parallel.ForEach`, `Parallel.Invoke`, `Task.Wait`,
    etc. The `AggregateException` type has some useful `Flatten` and `Handle` methods
    to simplify the error handling code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Usually, you don’t have to worry about how the work is handled by the thread
    pool. Data and task parallelism use dynamically adjusting partitioners to divide
    work among worker threads. The thread pool increases its thread count as necessary.
    The thread pool has a single work queue, and each threadpool thread also has its
    own work queue. When a threadpool thread queues additional work, it sends it to
    its own queue first because the work is usually related to the current work item;
    this behavior encourages threads to work on their own work, and maximizes cache
    hits. If another thread doesn’t have work to do, it’ll steal work from another
    thread’s queue. Microsoft put a lot of work into making the thread pool as efficient
    as possible, and there are a large number of knobs you can tweak if you need maximum
    performance. As long as your tasks are not extremely short, they should work well
    with the default settings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tasks should neither be extremely short, nor extremely long.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: If your tasks are too short, then the overhead of breaking up the data into
    tasks and scheduling those tasks on the thread pool becomes significant. If your
    tasks are too long, then the thread pool cannot dynamically adjust its work balancing
    efficiently. It’s difficult to determine how short is too short and how long is
    too long; it really depends on the problem being solved and the approximate capabilities
    of the hardware. As a general rule, I try to make my tasks as short as possible
    without running into performance issues (you’ll see your performance suddenly
    degrade when your tasks are too short). Even better, instead of using tasks directly,
    use the `Parallel` type or PLINQ. These higher-level forms of parallelism have
    partitioning built in to handle this automatically for you (and adjust as necessary
    at runtime).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: If you want to dive deeper into parallel programming, the best book on the subject
    is *Parallel Programming with Microsoft .NET*, by Colin Campbell et al. (Microsoft
    Press).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Reactive Programming (Rx)
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reactive programming has a higher learning curve than other forms of concurrency,
    and the code can be harder to maintain unless you keep up with your reactive skills.
    If you’re willing to learn it, though, reactive programming is extremely powerful.
    Reactive programming enables you to treat a stream of events like a stream of
    data. As a rule of thumb, if you use any of the event arguments passed to an event,
    then your code would benefit from using System.Reactive instead of a regular event
    handler.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: System.Reactive used to be called Reactive Extensions, which was often shortened
    to “Rx.” All three of these terms refer to the same technology.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Reactive programming is based on the notion of observable streams. When you
    subscribe to an observable stream, you’ll receive any number of data items (`OnNext`),
    and then the stream may end with a single error (`OnError`) or “end of stream”
    notification (`OnCompleted`). Some observable streams never end. The actual interfaces
    look like the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 响应式编程基于可观察流的概念。当您订阅一个可观察流时，您将接收任意数量的数据项 (`OnNext`)，然后流可能以单个错误 (`OnError`) 或
    “流结束” 通知 (`OnCompleted`) 结束。某些可观察流永远不会结束。实际的接口如下所示：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'However, you should never implement these interfaces. The System.Reactive (Rx)
    library by Microsoft has all the implementations you should ever need. Reactive
    code ends up looking very much like LINQ; you can think of it as “LINQ to Events.”
    System.Reactive has everything that LINQ does and adds in a large number of its
    own operators, particularly ones that deal with time. The following code starts
    with some unfamiliar operators (`Interval` and `Timestamp`) and ends with a `Subscribe`,
    but in the middle are some `Where` and `Select` operators that should be familiar
    from LINQ:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您永远不应该实现这些接口。Microsoft 的 System.Reactive (Rx) 库已经包含了您可能需要的所有实现。响应式代码看起来非常类似于
    LINQ；您可以将其视为 “LINQ to Events”。System.Reactive 拥有与 LINQ 相同的所有功能，并添加了大量自己的操作符，特别是处理时间的操作符。以下代码从一些不熟悉的操作符
    (`Interval` 和 `Timestamp`) 开始，以 `Subscribe` 结束，但中间有一些您应该从 LINQ 中熟悉的 `Where` 和
    `Select` 操作符：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The example code starts with a counter running off a periodic timer (`Interval`)
    and adds a timestamp to each event (`Timestamp`). It then filters the events to
    only include even counter values (`Where`), selects the timestamp values (`Timestamp`),
    and then as each resulting timestamp value arrives, writes it to the debugger
    (`Subscribe`). Don’t worry if you don’t understand the new operators, such as
    `Interval`: these are covered later in this book. For now, just keep in mind that
    this is a LINQ query very similar to the ones you’re already familiar with. The
    main difference is that LINQ to Objects and LINQ to Entities use a *“pull” model*,
    where the enumeration of a LINQ query pulls the data through the query, while
    LINQ to Events (System.Reactive) uses a *“push” model*, where the events arrive
    and travel through the query by themselves.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码从一个周期性定时器 (`Interval`) 开始运行一个计数器，并为每个事件添加时间戳 (`Timestamp`)。然后，它会过滤事件，只包括偶数计数器值
    (`Where`)，选择时间戳值 (`Timestamp`)，最后每个结果的时间戳值到达时，将其写入调试器 (`Subscribe`)。如果您不理解 `Interval`
    等新操作符，不要担心：这些稍后在本书中会进行介绍。现在只需记住，这是一个与您已经熟悉的 LINQ 查询非常相似的 LINQ 查询。主要区别在于 LINQ to
    Objects 和 LINQ to Entities 使用 *“拉取”模型*，即 LINQ 查询的枚举通过查询拉取数据，而 LINQ to Events (System.Reactive)
    使用 *“推送”模型*，即事件到达并自行通过查询传递。
- en: 'The definition of an observable stream is independent from its subscriptions.
    The last example is the same as the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察流的定义与其订阅是独立的。最后的例子与以下代码相同：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is normal for a type to define the observable streams and make them available
    as an `IObservable<TResult>` resource. Other types can then subscribe to those
    streams or combine them with other operators to create another observable stream.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 类型定义可观察流并将其作为 `IObservable<TResult>` 资源提供是正常的。其他类型可以订阅这些流或与其他操作符结合以创建另一个可观察流。
- en: A System.Reactive subscription is also a resource. The `Subscribe` operators
    return an `IDisposable` that represents the subscription. When your code is done
    listening to an observable stream, it should dispose its subscription.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`System.Reactive` 的订阅也是一种资源。`Subscribe` 操作符返回一个 `IDisposable`，表示订阅。当您的代码完成对可观察流的监听后，应该释放其订阅。'
- en: Subscriptions behave differently with hot and cold observables. A *hot observable*
    is a stream of events that is always going on, and if there are no subscribers
    when the events come in, they are lost. For example, mouse movement is a hot observable.
    A *cold observable* is an observable that doesn’t have incoming events all the
    time. A cold observable will react to a subscription by starting the sequence
    of events. For example, an HTTP download is a cold observable; the subscription
    causes the HTTP request to be sent.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用热和冷可观察流时，订阅的行为是不同的。*热可观察流* 是一个始终运行的事件流，如果没有订阅者在事件到达时，它们将丢失。例如，鼠标移动是一个热可观察流。*冷可观察流*
    是不会始终有传入事件的可观察流。冷可观察流将通过订阅启动事件序列。例如，HTTP 下载是一个冷可观察流；订阅导致发送 HTTP 请求。
- en: 'The `Subscribe` operator should always take an error handling parameter as
    well. The preceding examples do not; the following is a better example that will
    respond appropriately if the observable stream ends in an error:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Subject<TResult>` is one type that is useful when experimenting with System.Reactive.
    This “subject” is like a manual implementation of an observable stream. Your code
    can call `OnNext`, `OnError`, and `OnCompleted`, and the subject will forward
    those calls to its subscribers. `Subject<TResult>` is great for experimenting,
    but in production code, you should strive to use operators like those covered
    in [Chapter 6](ch06.html#rx-basics).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: There are tons of useful System.Reactive operators, and I only cover a few selected
    ones in this book. For more information on System.Reactive, I recommend the excellent
    online book [*Introduction to Rx*](http://www.introtorx.com/).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Dataflows
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TPL Dataflow is an interesting mix of asynchronous and parallel technologies.
    It’s useful when you have a sequence of processes that need to be applied to your
    data. For example, you may need to download data from a URL, parse it, and then
    process it in parallel with other data. TPL Dataflow is commonly used as a simple
    pipeline, where data enters one end and travels until it comes out the other.
    However, TPL Dataflow is far more powerful than this; it’s capable of handling
    any kind of mesh. You can define forks, joins, and loops in a mesh, and TPL Dataflow
    will handle them appropriately. Most of the time, though, TPL Dataflow meshes
    are used as a pipeline.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The basic building unit of a dataflow mesh is a *dataflow block*. A block can
    either be a target block (receiving data), a source block (producing data), or
    both. Source blocks can be linked to target blocks to create the mesh; linking
    is covered in [Recipe 5.1](ch05.html#recipe-dataflow-linking). Blocks are semi-independent;
    they will attempt to process data as it arrives and push the results downstream.
    The usual way of using TPL Dataflow is to create all the blocks, link them together,
    and then start putting data in at one end. The data then comes out of the other
    end by itself. Again, Dataflow is more powerful than this; it’s possible to break
    links and create new blocks and add them to the mesh *while* there is data flowing
    through it, but that is a very advanced scenario.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Target blocks have buffers for the data they receive. Having buffers enables
    them to accept new data items even if they aren’t ready to process them yet; this
    keeps data flowing through the mesh. This buffering can cause problems in fork
    scenarios, where one source block is linked to two target blocks. When the source
    block has data to send downstream, it starts offering it to its linked blocks
    one at a time. By default, the first target block would just take the data and
    buffer it, and the second target block would never get any. The fix for this situation
    is to limit the target block buffers by making them nongreedy; [Recipe 5.4](ch05.html#recipe-dataflow-throttling)
    covers this.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目标块具有用于接收它们接收的数据的缓冲区。通过具有缓冲区，即使它们还没有准备好处理数据项，它们也能够接受新的数据项；这保持了数据通过网格的流动。这种缓冲可能会在分叉场景中引发问题，其中一个源块链接到两个目标块。当源块有数据要发送到下游时，它开始逐个向其链接的块提供数据。默认情况下，第一个目标块只会接受数据并缓冲它，而第二个目标块则永远不会得到任何数据。解决此情况的方法是通过使目标块成为非贪婪的来限制目标块的缓冲区；[菜谱
    5.4](ch05.html#recipe-dataflow-throttling)详细介绍了这一点。
- en: 'A block will fault when something goes wrong, for example, if the processing
    delegate throws an exception when processing a data item. When a block faults,
    it will stop receiving data. By default, it won’t take down the whole mesh; this
    enables you to rebuild that part of the mesh or redirect the data. However, this
    is an advanced scenario; most times, you want the faults to propagate along the
    links to the target blocks. Dataflow supports this option as well; the only tricky
    part is that when an exception is propagated along a link, it is wrapped in an
    `AggregateException`. So, if you have a long pipeline, you could end up with a
    deeply nested exception; the method `AggregateException.Flatten` can be used to
    work around this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当某个块出现问题时，例如在处理数据项时处理委托引发异常时，该块将出现故障。当块出现故障时，它将停止接收数据。默认情况下，它不会导致整个网格崩溃；这使您能够重建网格的那一部分或重定向数据。但是，这是一个高级场景；大多数情况下，您希望故障沿着链路传播到目标块。数据流也支持此选项；唯一棘手的部分是当异常沿链路传播时，它会被包装在`AggregateException`中。因此，如果您有一个长的管道，可能会出现深度嵌套的异常；方法`AggregateException.Flatten`可用于解决此问题：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Recipe 5.2](ch05.html#recipe-dataflow-errors) covers dataflow error handling
    in more detail.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[菜谱 5.2](ch05.html#recipe-dataflow-errors)详细介绍了数据流错误处理。'
- en: 'At first glance, dataflow meshes sound very much like observable streams, and
    they do have much in common. Both meshes and streams have the concept of data
    items passing through them. Also, both meshes and streams have the notion of a
    normal completion (a notification that no more data is coming), as well as a faulting
    completion (a notification that some error occurred during data processing). But
    System.Reactive (Rx) and TPL Dataflow do not have the same capabilities. Rx observables
    are generally better than dataflow blocks when doing anything related to timing.
    Dataflow blocks are generally better than Rx observables when doing parallel processing.
    Conceptually, Rx works more like setting up callbacks: each step in the observable
    directly calls the next step. In contrast, each block in a dataflow mesh is very
    independent from all the other blocks. Both Rx and TPL Dataflow have their own
    uses, with some amount of overlap. They also work quite well together; [Recipe
    8.8](ch08.html#recipe-rx-interop-dataflow) covers interoperability between Rx
    and TPL Dataflow.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，数据流网格听起来很像可观察流，它们确实有很多共同点。网格和流都有数据项通过的概念。而且，网格和流都有正常完成（通知没有更多数据到来）和故障完成（通知在数据处理过程中发生错误）的概念。但是，System.Reactive（Rx）和TPL
    Dataflow并没有相同的能力。在处理与时间相关的任何事务时，Rx可观察流通常比数据流块更好。而在进行并行处理时，数据流块通常比Rx可观察流更好。从概念上讲，Rx更像是设置回调：可观察流中的每个步骤直接调用下一个步骤。相比之下，数据流网格中的每个块都与所有其他块非常独立。Rx和TPL
    Dataflow都有各自的用途，并存在一定的重叠。它们在一起工作也非常好；[菜谱 8.8](ch08.html#recipe-rx-interop-dataflow)详细介绍了Rx和TPL
    Dataflow之间的互操作性。
- en: If you’re familiar with actor frameworks, TPL Dataflow will seem to share similarities
    with them. Each dataflow block is independent, in the sense that it will spin
    up tasks to do work as needed, like executing a transformation delegate or pushing
    output to the next block. You can also set up each block to run in parallel, so
    that it’ll spin up multiple tasks to deal with additional input. Due to this behavior,
    each block does have a certain similarity to an actor in an actor framework. However,
    TPL Dataflow is not a full actor framework; in particular, there’s no built-in
    support for clean error recovery or retries of any kind. TPL Dataflow is a library
    with an actor-like feel, but it isn’t a full-featured actor framework.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The most common TPL Dataflow block types are `TransformBlock<TInput, TOutput>`
    (similar to LINQ’s `Select`), `TransformManyBlock<TInput, TOutput>` (similar to
    LINQ’s `SelectMany`), and `ActionBlock<TResult>`, which executes a delegate for
    each data item. For more information on TPL Dataflow, I recommend the [MSDN documentation](http://bit.ly/dataflow-doc)
    and the [“Guide to Implementing Custom TPL Dataflow Blocks”](http://bit.ly/tpl-dataflow).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Multithreaded Programming
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *thread* is an independent executor. Each process has multiple threads in
    it, and each of those threads can be doing different things simultaneously. Each
    thread has its own independent stack but shares the same memory with all the other
    threads in a process. In some applications, there is one thread that is special.
    For example, user interface applications have a single special UI thread, and
    Console applications have a single special main thread.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Every .NET application has a thread pool. The thread pool maintains a number
    of worker threads that are waiting to execute whatever work you have for them
    to do. The thread pool is responsible for determining how many threads are in
    the thread pool at any time. There are dozens of configuration settings you can
    play with to modify this behavior, but I recommend that you leave it alone; the
    thread pool has been carefully tuned to cover the vast majority of real-world
    scenarios.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: There is almost no need for you to ever create a new thread yourself. The only
    time you should ever create a `Thread` instance is if you need an STA thread for
    COM interop.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread is a low-level abstraction. The thread pool is a slightly higher level
    of abstraction; when code queues work to the thread pool, the thread pool itself
    will take care of creating a thread if necessary. The abstractions covered in
    this book are higher still: parallel and dataflow processing queues work to the
    thread pool as necessary. Code using these higher abstractions is easier to get
    right than code using low-level abstractions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the `Thread` and `BackgroundWorker` types are not covered at
    all in this book. They have had their time, and that time is over.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Collections for Concurrent Applications
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a couple of collection categories that are useful for concurrent
    programming: concurrent collections and immutable collections. Both of these collection
    categories are covered in [Chapter 9](ch09.html#collections). Concurrent collections
    allow multiple threads to update them simultaneously in a safe way. Most concurrent
    collections use *snapshots* to enable one thread to enumerate the values while
    another thread may be adding or removing values. Concurrent collections are usually
    more efficient than just protecting a regular collection with a lock.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Immutable collections are a bit different. An immutable collection cannot actually
    be modified; instead, to modify an immutable collection, you create a new collection
    that represents the modified collection. This sounds horribly inefficient, but
    immutable collections share as much memory as possible between collection instances,
    so it’s not as bad as it sounds. The nice thing about immutable collections is
    that all operations are pure, so they work very well with functional code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Modern Design
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most concurrent technologies have one similar aspect: they are functional in
    nature. I don’t mean *functional* as in “they get the job done,” but rather *functional*
    as a style of programming that is based on function composition. If you adopt
    a functional mindset, your concurrent designs will be less convoluted.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: One principle of functional programming is *purity* (that is, avoiding side
    effects). Each piece of the solution takes some value(s) as input and produces
    some value(s) as output. As much as possible, you should avoid having these pieces
    depend on global (or shared) variables or update global (or shared) data structures.
    This is true whether the piece is an `async` method, a parallel task, a System.Reactive
    operation, or a dataflow block. Of course, sooner or later your computations will
    have to have an effect, but you’ll find your code is cleaner if you can handle
    the *processing* with pure pieces and then perform updates with the *results*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Another principle of functional programming is *immutability*. Immutability
    means that a piece of data cannot change. One reason that immutable data is useful
    for concurrent programs is that you never need synchronization for immutable data;
    the fact that it cannot change makes synchronization unnecessary. Immutable data
    also helps you avoid side effects. Developers are beginning to use more immutable
    types, and this book has several recipes covering immutable data structures.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Key Technologies
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The .NET framework has had some support for asynchronous programming since the
    very beginning. However, asynchronous programming was difficult until 2012, when
    .NET 4.5 (along with C# 5.0 and VB 2012) introduced the `async` and `await` keywords.
    This book will use the modern `async`/`await` approach for all asynchronous recipes,
    and it has some recipes showing how to interoperate between `async` and the older
    asynchronous programming patterns. If you need support for older platforms, see
    [Appendix A](app01.html#legacy-platform-support).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: .NET框架自始至终对异步编程有一些支持。然而，直到2012年，也就是.NET 4.5（连同C# 5.0和VB 2012）引入了`async`和`await`关键字之前，异步编程一直很困难。本书将使用现代的`async`/`await`方法来处理所有异步任务，并提供了一些示例，展示如何在`async`和旧的异步编程模式之间进行交互。如果需要支持旧平台，请参阅[附录 A](app01.html#legacy-platform-support)。
- en: The Task Parallel Library was introduced in .NET 4.0 with full support for both
    data and task parallelism. These days, it’s available even on platforms with fewer
    resources, such as mobile phones. The TPL is built in to .NET.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: .NET 4.0引入了任务并行库（Task Parallel Library，TPL），全面支持数据并行和任务并行。如今，即使在资源较少的平台如手机上，也可以使用。TPL已经内置于.NET中。
- en: The System.Reactive team has worked hard to support as many platforms as possible.
    System.Reactive, like `async` and `await`, provide benefits for all sorts of applications,
    both client and server. System.Reactive is available in the [`System.Reactive`](http://bit.ly/sys-reactive)
    NuGet package.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: System.Reactive团队努力支持尽可能多的平台。像`async`和`await`一样，System.Reactive为各种应用程序（包括客户端和服务器）提供了诸多好处。System.Reactive可以在[`System.Reactive`](http://bit.ly/sys-reactive)
    NuGet包中找到。
- en: The TPL Dataflow library is officially distributed within the NuGet package
    for [`System.Threading.Tasks.Dataflow`](http://bit.ly/nuget-df).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: TPL Dataflow库正式分发在[`System.Threading.Tasks.Dataflow`](http://bit.ly/nuget-df)
    NuGet包中。
- en: Most concurrent collections are built into .NET; there are some additional concurrent
    collections available in the [`System.Threading.Channels`](https://www.nuget.org/packages/System.Threading.Channels)
    NuGet package. Immutable collections are available in the [`System.Collections.Immutable`](http://bit.ly/sys-coll-imm)
    NuGet package.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数并发集合都内置于.NET中；[`System.Threading.Channels`](https://www.nuget.org/packages/System.Threading.Channels)
    NuGet包中还提供了一些额外的并发集合。不可变集合则可以在[`System.Collections.Immutable`](http://bit.ly/sys-coll-imm)
    NuGet包中找到。
